{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://habr.com/ru/post/347564/.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "#os.environ['THEANO_FLAGS'] = \"mode=FAST_RUN,device=cuda:0,floatX=float32\"\n",
    "#import theano\n",
    "#print(theano.config.device, theano.config.floatX)\n",
    "\n",
    "import keras\n",
    "import PIL\n",
    "import sys\n",
    "#from google.colab import files # для импорта данных в google colab\n",
    "#from google.colab import drive # для импорта данных в google colab из google drive\n",
    "import zipfile # для работы с архивами \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import cv2\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.applications import xception\n",
    "#from keras.applications import resnet\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from tensorflow.keras.preprocessing import image as im\n",
    "import time\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import densenet\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10513024560051973327\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16181014659712759170\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 32070087476\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3325159402369502349\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:04:00.0, compute capability: 7.0\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4833630928692382115\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "class_names = ['1', '2', '3', '4']\n",
    "print(len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 71, 71\n",
    "nb_train_samples = 30451\n",
    "nb_validation_samples = 7434\n",
    "epochs = 12\n",
    "batch_size = 8\n",
    "n_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "im = cv2.imread('home/z_andrei/zoohackathon/i_cropped_train/1/32886.jpg')\n",
    "print(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"/home/z_andrei/zoohackathon/i_cropped_train\"    # дальше там папки, а не сами фотографии!!!!\n",
    "validation_data_dir = \"/home/z_andrei/zoohackathon/i_cropped_test\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    #shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    #fill_mode = 'constant',\n",
    "    #cval = 1,\n",
    "    rotation_range = 5,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip = False,\n",
    "    vertical_flip = False)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1.0 / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30451 images belonging to 4 classes.\n",
      "Found 7434 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_Xception():\n",
    "  base_model = xception.Xception(include_top=True, \n",
    "                                      weights = None,\n",
    "                                      input_tensor = None,\n",
    "                                      input_shape=(img_width, img_height, 3),\n",
    "                                      pooling=None,\n",
    "                                      classes = 4)\n",
    "  \n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "      \n",
    "  model = Model(inputs = base_model.input, outputs = base_model.output)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model_Xception()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=8, verbose=1, min_delta=1e-4) # остановка обучения, если loss на валидационном множесте улучшается менее чем на 10^-4\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, verbose=1, min_delta=1e-4)\n",
    "filepath=\"weights-zoo-128-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "check = ModelCheckpoint(filepath, monitor = \"val_acc\", save_best_only = False) # сохранение лучшей (с наибольшим acc на валидационном множестве) сети\n",
    "callbacks_list = [early_stop, reduce_lr , check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "3806/3806 [==============================] - 367s 96ms/step - loss: 0.4795 - acc: 0.8372 - mse: 0.0589 - val_loss: 0.1898 - val_acc: 0.8924 - val_mse: 0.0395\n",
      "Epoch 2/12\n",
      "3806/3806 [==============================] - 362s 95ms/step - loss: 0.3381 - acc: 0.8842 - mse: 0.0424 - val_loss: 0.0204 - val_acc: 0.9185 - val_mse: 0.0301\n",
      "Epoch 3/12\n",
      "3806/3806 [==============================] - 341s 90ms/step - loss: 0.2770 - acc: 0.9030 - mse: 0.0356 - val_loss: 0.0783 - val_acc: 0.9333 - val_mse: 0.0252\n",
      "Epoch 4/12\n",
      "3806/3806 [==============================] - 349s 92ms/step - loss: 0.2345 - acc: 0.9174 - mse: 0.0305 - val_loss: 0.0933 - val_acc: 0.9185 - val_mse: 0.0306\n",
      "Epoch 5/12\n",
      "3806/3806 [==============================] - 361s 95ms/step - loss: 0.2098 - acc: 0.9272 - mse: 0.0273 - val_loss: 0.0166 - val_acc: 0.9362 - val_mse: 0.0231\n",
      "Epoch 6/12\n",
      "3806/3806 [==============================] - 359s 94ms/step - loss: 0.1864 - acc: 0.9357 - mse: 0.0243 - val_loss: 0.1380 - val_acc: 0.9519 - val_mse: 0.0182\n",
      "Epoch 7/12\n",
      "3806/3806 [==============================] - 3193s 839ms/step - loss: 0.1718 - acc: 0.9404 - mse: 0.0226 - val_loss: 0.0550 - val_acc: 0.9506 - val_mse: 0.0177\n",
      "Epoch 8/12\n",
      "3806/3806 [==============================] - 358s 94ms/step - loss: 0.1606 - acc: 0.9444 - mse: 0.0211 - val_loss: 0.2498 - val_acc: 0.9554 - val_mse: 0.0169\n",
      "Epoch 9/12\n",
      "3806/3806 [==============================] - 324s 85ms/step - loss: 0.1501 - acc: 0.9468 - mse: 0.0198 - val_loss: 0.0100 - val_acc: 0.9557 - val_mse: 0.0167\n",
      "Epoch 10/12\n",
      "3806/3806 [==============================] - 353s 93ms/step - loss: 0.1417 - acc: 0.9513 - mse: 0.0186 - val_loss: 0.0263 - val_acc: 0.9631 - val_mse: 0.0149\n",
      "Epoch 11/12\n",
      "3806/3806 [==============================] - 334s 88ms/step - loss: 0.1337 - acc: 0.9536 - mse: 0.0176 - val_loss: 0.0312 - val_acc: 0.9615 - val_mse: 0.0145\n",
      "Epoch 12/12\n",
      "3806/3806 [==============================] - 341s 89ms/step - loss: 0.1322 - acc: 0.9542 - mse: 0.0174 - val_loss: 0.0178 - val_acc: 0.9632 - val_mse: 0.0135\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs = epochs,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size,\n",
    "    callbacks = callbacks_list,\n",
    "    steps_per_epoch = nb_train_samples // batch_size )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
